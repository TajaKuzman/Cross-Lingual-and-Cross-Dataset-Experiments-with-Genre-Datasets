{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"In this notebook, we will use the American-British-variety classifier (https://github.com/macocu/American-British-variety-classifier) to analyse which English variety is more frequent in the dataset.\n\nTo use the classifier in Kaggle, we need to upload the lexicon.pickle file from the GitHub repository (https://github.com/macocu/American-British-variety-classifier/blob/main/lexicon.pickle) and copy the main code from https://github.com/macocu/American-British-variety-classifier/blob/main/ABClf.py","metadata":{}},{"cell_type":"code","source":"# Import and install all necessary libraries\n!pip install parse\n!pip install transliterate\n\n","metadata":{"execution":{"iopub.status.busy":"2022-03-15T09:41:45.680922Z","iopub.execute_input":"2022-03-15T09:41:45.681202Z","iopub.status.idle":"2022-03-15T09:41:45.685745Z","shell.execute_reply.started":"2022-03-15T09:41:45.681174Z","shell.execute_reply":"2022-03-15T09:41:45.684923Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport pickle\nimport logging\nfrom parse import compile\nfrom transliterate import translit\nfrom typing import Set, List, Dict","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Copy the code from the ABClf.py\n# in https://github.com/macocu/American-British-variety-classifier\n\nchars_to_remove = {\n    '!',\n    '\"',\n    '#',\n    '%',\n    '&',\n    '(',\n    ')',\n    '*',\n    '+',\n    ',',\n    '-',\n    '.',\n    '/',\n    ':',\n    ';',\n    '<',\n    '=',\n    '>',\n    '?',\n    '[',\n    ']',\n    '_',\n    '`',\n    '«',\n    '°',\n    '²',\n    '³',\n    'µ',\n    '·',\n    '»',\n    '½',\n    '‑',\n    '–',\n    '‘',\n    '’',\n    '“',\n    '”',\n    '„',\n    '•',\n    '…',\n    '‰',\n    '″',\n    '₂',\n    '₃',\n    '€',\n    '™',\n    '→',\n    '−',\n    '∕',\n    '😀',\n    '😉',\n    '🙁',\n    '🙂'\n\n}\n\n\ndef is_alpha(token: str) -> bool:\n    \"\"\"Checks if the input string is strictly lowercase without numerals.\n    Args:\n        token (str): Input text.\n    Returns:\n        bool: Result of checking.\n    \"\"\"    \n    import re\n    pattern = \"^[a-zšđčćž]+$\"\n    compiled_pattern = re.compile(pattern)\n    return bool(compiled_pattern.match(token))\n\ndef preprocess(s: str) -> str:\n    \"\"\"Removes unusual characters and lowercases the string.\n    Args:\n        s (str): input string.\n    Returns:\n        str: output string.\n    \"\"\"\n    for c in chars_to_remove:\n        s = s.replace(c, \"\")\n    s = s.casefold()\n    return s\n\n\ndef count_variants(s: str, lex: dict):\n    \"\"\"Counts the variant specific words in the preprocessed input string based on the lexicon lex.\n    Returns tuple (counts, per_token_breakdown).\n    Counts look like this:\n        {\"A\":3, \"B\":0}.\n    per_token is a dictionary with all the words detected, their counts and their variant:\n        {\"word1\":\n            {\"count\":3, \"variant\":\"A\"}\n        }\n    Args:\n        s (str): Input string.\n        lex (dict): Lexicon.\n    Returns:\n        results (tuple): (counts, per_token).\n    \"\"\"\n    counts = dict()\n    per_token = dict()\n    for word in preprocess(s).split():\n        if not is_alpha(word):\n            continue\n        variant = lex.get(word, None)\n        if not variant:\n            continue\n        logging.debug(f\"Found word {word}, presumed variant: {variant}.\")\n        counts[variant] = counts.get(variant, 0) + 1\n        if word in per_token.keys():\n            per_token[word][\"count\"] += 1\n        else:\n            per_token[word] = {\"variant\": variant, \"count\": 1}\n    return counts, per_token\n\n\n\ndef counts_to_category(counts: dict) -> str:\n    \"\"\"Discretizes counts like {\"A\": 2, \"B\":0} to\n    categories A, B, MIX, UNK.\n    Args:\n        counts (dict): result of count_variants function.\n    Returns:\n        str: category.\n    \"\"\"    \n    A = counts.get(\"A\", 0)\n    B = counts.get(\"B\",0)\n\n    if A > 2*B:\n        return \"A\"\n    elif B > 2*A:\n        return \"B\"\n    elif A == B == 0:\n        return \"UNK\"\n    else:\n        return \"MIX\"\n\n\ndef load_lexicon(balanced=False)-> dict:\n    \"\"\"Loads 'lexicon.pickle'.\n    Args:\n        balanced (bool, optional):\n        #Whether or not to use balanced lexicon (equal number of A and B keys).\n        Defaults to False. \n    Returns:\n        dict: lexicon for variety identification.\n    \"\"\" \n    with open(lexicon_path, \"rb\") as f:\n        lex = pickle.load(f)\n    return lex\n\ndef get_variant(text: str, lex=None) -> str:\n    \"\"\"Quick way to classify text. \n    Loads the lexicon, preprocesses the string. Returns the predicted \n    category {'A', 'B', 'UNK', 'MIX'}.\n    Args:\n        text (str): input string.\n    Returns:\n        str: category {'A', 'B', 'UNK', 'MIX'}\n    \"\"\"    \n    if not lex:\n        lex = load_lexicon()\n    variant_detector_count = count_variants(text, lex)[0]\n    return counts_to_category(variant_detector_count) ","metadata":{"execution":{"iopub.status.busy":"2022-03-15T10:00:10.294437Z","iopub.execute_input":"2022-03-15T10:00:10.294791Z","iopub.status.idle":"2022-03-15T10:00:10.319680Z","shell.execute_reply.started":"2022-03-15T10:00:10.294758Z","shell.execute_reply":"2022-03-15T10:00:10.318769Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# Define the path to the lexicon.pickle\nlexicon_path = \"/kaggle/input/english-variety-classifier-lexicon/lexicon.pickle\"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import the dataset you want to analyse. If the dataset is not in the format\n# that can be opened with pandas, transform it in a suitable format\n# and open it in a pandas DataFrame. The text should be in the column names 'text'.\n\ndf = pd.read_csv(\"/kaggle/input/corewholedataset/cleaned_CORE_corpora.csv\")\n\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-15T09:55:23.117033Z","iopub.execute_input":"2022-03-15T09:55:23.117779Z","iopub.status.idle":"2022-03-15T09:55:28.612518Z","shell.execute_reply.started":"2022-03-15T09:55:23.117727Z","shell.execute_reply":"2022-03-15T09:55:28.611590Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# In the next steps, variety will be identified and various relevant information\n# will be added to the dataframe.\n\nlex = load_lexicon()\n\n# Count how many words, detected in the text, are British, and how many American\ndf[\"variant_detector_count\"] = df.text.apply(lambda s: count_variants(s, lex)[0])\n\n# Provide more information, write out which words were detected\ndf[\"variant_detector_breakdown\"] = df.text.apply(lambda s: count_variants(s, lex)[1])\n\n# Count number of all words in the text\ndf[\"words\"] = df.text.apply(lambda t: len(t.split()))\n\n# Calculate the difference between no. of British words and no. of American words.\n# If there is less than twice as many words of one variant than words of the other variant,\n# text is classified as \"MIX\"\ndf[\"A_B\"] = df.variant_detector_count.apply(lambda d:d.get(\"A\", 0) - d.get(\"B\", 0))\n\n# Classify text as British (B), American (A),\n# Mixed (MIX) or Unknown (UNK - contains no variety-specific word)\ndf[\"variant\"] = df.variant_detector_count.apply(counts_to_category)","metadata":{"execution":{"iopub.status.busy":"2022-03-15T10:00:16.568728Z","iopub.execute_input":"2022-03-15T10:00:16.569711Z","iopub.status.idle":"2022-03-15T10:03:33.751585Z","shell.execute_reply.started":"2022-03-15T10:00:16.569655Z","shell.execute_reply":"2022-03-15T10:03:33.750273Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# See the resulting DataFrame\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-15T10:12:51.960106Z","iopub.execute_input":"2022-03-15T10:12:51.960475Z","iopub.status.idle":"2022-03-15T10:12:51.986660Z","shell.execute_reply.started":"2022-03-15T10:12:51.960439Z","shell.execute_reply":"2022-03-15T10:12:51.985295Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# See the distribution of variety classes in the dataset\ndf.variant.value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-03-15T10:13:51.155364Z","iopub.execute_input":"2022-03-15T10:13:51.155958Z","iopub.status.idle":"2022-03-15T10:13:51.171479Z","shell.execute_reply.started":"2022-03-15T10:13:51.155910Z","shell.execute_reply":"2022-03-15T10:13:51.170537Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"# Let's see the percentages of the values\ndf.variant.value_counts(normalize=True).mul(100).round(1).astype(str) + '%'","metadata":{"execution":{"iopub.status.busy":"2022-03-15T10:15:54.644793Z","iopub.execute_input":"2022-03-15T10:15:54.645416Z","iopub.status.idle":"2022-03-15T10:15:54.659502Z","shell.execute_reply.started":"2022-03-15T10:15:54.645364Z","shell.execute_reply":"2022-03-15T10:15:54.658694Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"# Write out the tables in markdown format\nprint(df.variant.value_counts().to_markdown())\n","metadata":{"execution":{"iopub.status.busy":"2022-03-15T10:42:22.232564Z","iopub.execute_input":"2022-03-15T10:42:22.233580Z","iopub.status.idle":"2022-03-15T10:42:22.246076Z","shell.execute_reply.started":"2022-03-15T10:42:22.233530Z","shell.execute_reply":"2022-03-15T10:42:22.245096Z"},"trusted":true},"execution_count":31,"outputs":[]}]}